{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# https://machinelearningmastery.com/multi-class-classification-tutorial-keras-deep-learning-library/\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Activation\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('processed_data.csv')\n",
    "df.set_index(['Ticker', 'Date'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = df.loc[833][35:]\n",
    "test.insert(0, 'Change_Indicator', np.where(test['ROCP_Close'] >= 0, 1, 0))\n",
    "dimensions = len(test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(df, sequence_length=10, split=0.8):\n",
    "    data_all = np.array(df).astype(float)\n",
    "    scaler = MinMaxScaler()  # What is this?\n",
    "    data_all = scaler.fit_transform(data_all)\n",
    "    data = []\n",
    "    for i in range(len(data_all) - sequence_length - 1):\n",
    "        data.append(data_all[i: i + sequence_length + 1])\n",
    "    reshaped_data = np.array(data).astype('float64')\n",
    "    # Do we want to be shuffling the data? Isn't this going to lose out on the sequentiality of it?\n",
    "    np.random.shuffle(reshaped_data)\n",
    "    \n",
    "    x = reshaped_data[:, :-1]\n",
    "    y = reshaped_data[:, -1]\n",
    "    split_boundary = int(reshaped_data.shape[0] * split)\n",
    "    train_x = x[: split_boundary]\n",
    "    test_x = x[split_boundary:]\n",
    "\n",
    "    train_y = y[: split_boundary,0]\n",
    "    test_y = y[split_boundary:,0]\n",
    "    return train_x, train_y, test_x, test_y, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    # input_dim是输入的train_x的最后一个维度，train_x的维度为(n_samples, time_steps, input_dim)\n",
    "    # https://keras.io/getting-started/sequential-model-guide/\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(43, input_dim=dimensions, return_sequences=True))\n",
    "    model.add(LSTM(86, return_sequences=False))\n",
    "    model.add(Dense(43))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    model.add(Activation('linear'))\n",
    "\n",
    "\n",
    "    model.compile(loss='mse', optimizer='rmsprop')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_x, train_y, test_x, test_y):\n",
    "    model = build_model()\n",
    "\n",
    "    try:\n",
    "        labels = to_categorical(train_y, num_classes=2)\n",
    "        model.fit(train_x, labels, batch_size=512, nb_epoch=300, validation_split=0.1)\n",
    "        predict = model.predict(test_x)\n",
    "        predict = np.reshape(predict, (predict.size, ))\n",
    "    except KeyboardInterrupt:\n",
    "        print(predict)\n",
    "        print(test_y)\n",
    "        \n",
    "    print(predict)\n",
    "    print(test_y)\n",
    "    \n",
    "    try:\n",
    "        indicator = list()\n",
    "        count = 0\n",
    "        for i in range(1, len(predict), 2):\n",
    "            count += 1\n",
    "            indicator.append(1 if predict[i] >= predict[i - 1] else 0)\n",
    "        \n",
    "        fig = plt.figure(1)\n",
    "        plt.plot(indicator, 'r*')\n",
    "        plt.plot(test_y, 'g.')\n",
    "        plt.legend(['predict', 'true'])\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    return predict, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tzvi\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "  \"\"\"\n",
      "C:\\Users\\Tzvi\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(43, return_sequences=True, input_shape=(None, 43))`\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, None, 43)          14964     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 86)                44720     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 43)                3741      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 88        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 63,513\n",
      "Trainable params: 63,513\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = build_model()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tzvi\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "  \"\"\"\n",
      "C:\\Users\\Tzvi\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(43, return_sequences=True, input_shape=(None, 43))`\n",
      "  \"\"\"\n",
      "C:\\Users\\Tzvi\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 838 samples, validate on 94 samples\n",
      "Epoch 1/300\n",
      "838/838 [==============================] - 2s 2ms/step - loss: 0.2513 - val_loss: 0.2520\n",
      "Epoch 2/300\n",
      "838/838 [==============================] - 0s 239us/step - loss: 0.2482 - val_loss: 0.2433\n",
      "Epoch 3/300\n",
      "838/838 [==============================] - 0s 239us/step - loss: 0.2455 - val_loss: 0.2454\n",
      "Epoch 4/300\n",
      "838/838 [==============================] - 0s 239us/step - loss: 0.2454 - val_loss: 0.2447\n",
      "Epoch 5/300\n",
      "838/838 [==============================] - 0s 239us/step - loss: 0.2449 - val_loss: 0.2442\n",
      "Epoch 6/300\n",
      "838/838 [==============================] - 0s 258us/step - loss: 0.2445 - val_loss: 0.2429\n",
      "Epoch 7/300\n",
      "838/838 [==============================] - 0s 221us/step - loss: 0.2449 - val_loss: 0.2411\n",
      "Epoch 8/300\n",
      "838/838 [==============================] - 0s 239us/step - loss: 0.2534 - val_loss: 0.2410\n",
      "Epoch 9/300\n",
      "838/838 [==============================] - 0s 239us/step - loss: 0.2456 - val_loss: 0.2416\n",
      "Epoch 10/300\n",
      "838/838 [==============================] - 0s 248us/step - loss: 0.2447 - val_loss: 0.2423\n",
      "Epoch 11/300\n",
      "838/838 [==============================] - 0s 277us/step - loss: 0.2445 - val_loss: 0.2441\n",
      "Epoch 12/300\n",
      "838/838 [==============================] - 0s 251us/step - loss: 0.2448 - val_loss: 0.2427\n",
      "Epoch 13/300\n",
      "838/838 [==============================] - 0s 305us/step - loss: 0.2444 - val_loss: 0.2435\n",
      "Epoch 14/300\n",
      "838/838 [==============================] - 0s 239us/step - loss: 0.2440 - val_loss: 0.2434\n",
      "Epoch 15/300\n",
      "838/838 [==============================] - 0s 263us/step - loss: 0.2441 - val_loss: 0.2420\n",
      "Epoch 16/300\n",
      "838/838 [==============================] - 0s 302us/step - loss: 0.2441 - val_loss: 0.2424\n",
      "Epoch 17/300\n",
      "838/838 [==============================] - 0s 252us/step - loss: 0.2461 - val_loss: 0.2412\n",
      "Epoch 18/300\n",
      "838/838 [==============================] - 0s 267us/step - loss: 0.2454 - val_loss: 0.2421\n",
      "Epoch 19/300\n",
      "838/838 [==============================] - 0s 239us/step - loss: 0.2445 - val_loss: 0.2458\n",
      "Epoch 20/300\n",
      "838/838 [==============================] - 0s 268us/step - loss: 0.2442 - val_loss: 0.2433\n",
      "Epoch 21/300\n",
      "838/838 [==============================] - 0s 251us/step - loss: 0.2439 - val_loss: 0.2434\n",
      "Epoch 22/300\n",
      "838/838 [==============================] - 0s 268us/step - loss: 0.2441 - val_loss: 0.2419\n",
      "Epoch 23/300\n",
      "838/838 [==============================] - 0s 247us/step - loss: 0.2448 - val_loss: 0.2433\n",
      "Epoch 24/300\n",
      "838/838 [==============================] - 0s 245us/step - loss: 0.2437 - val_loss: 0.2439\n",
      "Epoch 25/300\n",
      "838/838 [==============================] - 0s 223us/step - loss: 0.2458 - val_loss: 0.2471\n",
      "Epoch 26/300\n",
      "838/838 [==============================] - 0s 223us/step - loss: 0.2441 - val_loss: 0.2430\n",
      "Epoch 27/300\n",
      "838/838 [==============================] - 0s 253us/step - loss: 0.2445 - val_loss: 0.2418\n",
      "Epoch 28/300\n",
      "838/838 [==============================] - 0s 268us/step - loss: 0.2437 - val_loss: 0.2439\n",
      "Epoch 29/300\n",
      "838/838 [==============================] - 0s 332us/step - loss: 0.2435 - val_loss: 0.2431\n",
      "Epoch 30/300\n",
      "838/838 [==============================] - 0s 390us/step - loss: 0.2433 - val_loss: 0.2424\n",
      "Epoch 31/300\n",
      "838/838 [==============================] - 0s 271us/step - loss: 0.2440 - val_loss: 0.2464\n",
      "Epoch 32/300\n",
      "838/838 [==============================] - 0s 295us/step - loss: 0.2442 - val_loss: 0.2413\n",
      "Epoch 33/300\n",
      "838/838 [==============================] - 0s 261us/step - loss: 0.2436 - val_loss: 0.2434\n",
      "Epoch 34/300\n",
      "838/838 [==============================] - 0s 222us/step - loss: 0.2431 - val_loss: 0.2461\n",
      "Epoch 35/300\n",
      "838/838 [==============================] - 0s 229us/step - loss: 0.2430 - val_loss: 0.2424\n",
      "Epoch 36/300\n",
      "838/838 [==============================] - 0s 224us/step - loss: 0.2428 - val_loss: 0.2489\n",
      "Epoch 37/300\n",
      "838/838 [==============================] - 0s 224us/step - loss: 0.2440 - val_loss: 0.2437\n",
      "Epoch 38/300\n",
      "838/838 [==============================] - 0s 242us/step - loss: 0.2431 - val_loss: 0.2423\n",
      "Epoch 39/300\n",
      "838/838 [==============================] - 0s 231us/step - loss: 0.2429 - val_loss: 0.2439\n",
      "Epoch 40/300\n",
      "838/838 [==============================] - 0s 254us/step - loss: 0.2428 - val_loss: 0.2397\n",
      "Epoch 41/300\n",
      "838/838 [==============================] - 0s 241us/step - loss: 0.2457 - val_loss: 0.2417\n",
      "Epoch 42/300\n",
      "838/838 [==============================] - 0s 248us/step - loss: 0.2427 - val_loss: 0.2447\n",
      "Epoch 43/300\n",
      "838/838 [==============================] - 0s 254us/step - loss: 0.2436 - val_loss: 0.2460\n",
      "Epoch 44/300\n",
      "838/838 [==============================] - 0s 258us/step - loss: 0.2430 - val_loss: 0.2421\n",
      "Epoch 45/300\n",
      "838/838 [==============================] - 0s 246us/step - loss: 0.2422 - val_loss: 0.2442\n",
      "Epoch 46/300\n",
      "838/838 [==============================] - 0s 244us/step - loss: 0.2419 - val_loss: 0.2437\n",
      "Epoch 47/300\n",
      "838/838 [==============================] - 0s 231us/step - loss: 0.2419 - val_loss: 0.2430\n",
      "Epoch 48/300\n",
      "838/838 [==============================] - 0s 284us/step - loss: 0.2418 - val_loss: 0.2421\n",
      "Epoch 49/300\n",
      "838/838 [==============================] - 0s 299us/step - loss: 0.2414 - val_loss: 0.2479\n",
      "Epoch 50/300\n",
      "838/838 [==============================] - 0s 301us/step - loss: 0.2412 - val_loss: 0.2441\n",
      "Epoch 51/300\n",
      "838/838 [==============================] - 0s 305us/step - loss: 0.2424 - val_loss: 0.2477\n",
      "Epoch 52/300\n",
      "838/838 [==============================] - 0s 296us/step - loss: 0.2422 - val_loss: 0.2437\n",
      "Epoch 53/300\n",
      "838/838 [==============================] - 0s 382us/step - loss: 0.2441 - val_loss: 0.2497\n",
      "Epoch 54/300\n",
      "838/838 [==============================] - 0s 274us/step - loss: 0.2427 - val_loss: 0.2437\n",
      "Epoch 55/300\n",
      "838/838 [==============================] - 0s 242us/step - loss: 0.2409 - val_loss: 0.2444\n",
      "Epoch 56/300\n",
      "838/838 [==============================] - 0s 234us/step - loss: 0.2408 - val_loss: 0.2422\n",
      "Epoch 57/300\n",
      "838/838 [==============================] - 0s 295us/step - loss: 0.2408 - val_loss: 0.2431\n",
      "Epoch 58/300\n",
      "838/838 [==============================] - 0s 272us/step - loss: 0.2413 - val_loss: 0.2455\n",
      "Epoch 59/300\n",
      "838/838 [==============================] - 0s 240us/step - loss: 0.2413 - val_loss: 0.2449\n",
      "Epoch 60/300\n",
      "838/838 [==============================] - 0s 250us/step - loss: 0.2409 - val_loss: 0.2420\n",
      "Epoch 61/300\n",
      "838/838 [==============================] - 0s 230us/step - loss: 0.2422 - val_loss: 0.2486\n",
      "Epoch 62/300\n",
      "838/838 [==============================] - 0s 218us/step - loss: 0.2464 - val_loss: 0.2458\n",
      "Epoch 63/300\n",
      "838/838 [==============================] - 0s 246us/step - loss: 0.2408 - val_loss: 0.2426\n",
      "Epoch 64/300\n",
      "838/838 [==============================] - 0s 279us/step - loss: 0.2401 - val_loss: 0.2413\n",
      "Epoch 65/300\n",
      "838/838 [==============================] - 0s 244us/step - loss: 0.2398 - val_loss: 0.2416\n",
      "Epoch 66/300\n",
      "838/838 [==============================] - 0s 290us/step - loss: 0.2394 - val_loss: 0.2419\n",
      "Epoch 67/300\n",
      "838/838 [==============================] - 0s 230us/step - loss: 0.2391 - val_loss: 0.2419\n",
      "Epoch 68/300\n",
      "838/838 [==============================] - 0s 265us/step - loss: 0.2404 - val_loss: 0.2427\n",
      "Epoch 69/300\n",
      "838/838 [==============================] - 0s 271us/step - loss: 0.2394 - val_loss: 0.2412\n",
      "Epoch 70/300\n",
      "838/838 [==============================] - 0s 218us/step - loss: 0.2385 - val_loss: 0.2438\n",
      "Epoch 71/300\n",
      "838/838 [==============================] - 0s 291us/step - loss: 0.2396 - val_loss: 0.2469\n",
      "Epoch 72/300\n",
      "838/838 [==============================] - 0s 234us/step - loss: 0.2385 - val_loss: 0.2429\n",
      "Epoch 73/300\n",
      "838/838 [==============================] - 0s 278us/step - loss: 0.2398 - val_loss: 0.2500\n",
      "Epoch 74/300\n",
      "838/838 [==============================] - 0s 231us/step - loss: 0.2390 - val_loss: 0.2440\n",
      "Epoch 75/300\n",
      "838/838 [==============================] - 0s 273us/step - loss: 0.2381 - val_loss: 0.2423\n",
      "Epoch 76/300\n",
      "838/838 [==============================] - 0s 285us/step - loss: 0.2378 - val_loss: 0.2436\n",
      "Epoch 77/300\n",
      "838/838 [==============================] - 0s 297us/step - loss: 0.2381 - val_loss: 0.2428\n",
      "Epoch 78/300\n",
      "838/838 [==============================] - 0s 253us/step - loss: 0.2378 - val_loss: 0.2427\n",
      "Epoch 79/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "838/838 [==============================] - 0s 246us/step - loss: 0.2372 - val_loss: 0.2449\n",
      "Epoch 80/300\n",
      "838/838 [==============================] - 0s 250us/step - loss: 0.2393 - val_loss: 0.2468\n",
      "Epoch 81/300\n",
      "838/838 [==============================] - 0s 339us/step - loss: 0.2398 - val_loss: 0.2469\n",
      "Epoch 82/300\n",
      "838/838 [==============================] - 0s 248us/step - loss: 0.2369 - val_loss: 0.2444\n",
      "Epoch 83/300\n",
      "838/838 [==============================] - 0s 249us/step - loss: 0.2358 - val_loss: 0.2433\n",
      "Epoch 84/300\n",
      "838/838 [==============================] - 0s 272us/step - loss: 0.2367 - val_loss: 0.2451\n",
      "Epoch 85/300\n",
      "838/838 [==============================] - 0s 281us/step - loss: 0.2370 - val_loss: 0.2388\n",
      "Epoch 86/300\n",
      "838/838 [==============================] - 0s 254us/step - loss: 0.2380 - val_loss: 0.2434\n",
      "Epoch 87/300\n",
      "838/838 [==============================] - 0s 279us/step - loss: 0.2376 - val_loss: 0.2410\n",
      "Epoch 88/300\n",
      "838/838 [==============================] - 0s 252us/step - loss: 0.2357 - val_loss: 0.2443\n",
      "Epoch 89/300\n",
      "838/838 [==============================] - 0s 278us/step - loss: 0.2385 - val_loss: 0.2493\n",
      "Epoch 90/300\n",
      "838/838 [==============================] - 0s 241us/step - loss: 0.2383 - val_loss: 0.2477\n",
      "Epoch 91/300\n",
      "838/838 [==============================] - 0s 332us/step - loss: 0.2364 - val_loss: 0.2430\n",
      "Epoch 92/300\n",
      "838/838 [==============================] - 0s 274us/step - loss: 0.2357 - val_loss: 0.2420\n",
      "Epoch 93/300\n",
      "838/838 [==============================] - 0s 261us/step - loss: 0.2358 - val_loss: 0.2411\n",
      "Epoch 94/300\n",
      "838/838 [==============================] - 0s 310us/step - loss: 0.2354 - val_loss: 0.2439\n",
      "Epoch 95/300\n",
      "838/838 [==============================] - 0s 326us/step - loss: 0.2349 - val_loss: 0.2417\n",
      "Epoch 96/300\n",
      "838/838 [==============================] - 0s 296us/step - loss: 0.2365 - val_loss: 0.2466\n",
      "Epoch 97/300\n",
      "838/838 [==============================] - 0s 289us/step - loss: 0.2366 - val_loss: 0.2418\n",
      "Epoch 98/300\n",
      "838/838 [==============================] - 0s 343us/step - loss: 0.2339 - val_loss: 0.2456\n",
      "Epoch 99/300\n",
      "838/838 [==============================] - 0s 306us/step - loss: 0.2336 - val_loss: 0.2442\n",
      "Epoch 100/300\n",
      "838/838 [==============================] - 0s 304us/step - loss: 0.2329 - val_loss: 0.2434\n",
      "Epoch 101/300\n",
      "838/838 [==============================] - 0s 272us/step - loss: 0.2340 - val_loss: 0.2492\n",
      "Epoch 102/300\n",
      "838/838 [==============================] - 0s 286us/step - loss: 0.2362 - val_loss: 0.2459\n",
      "Epoch 103/300\n",
      "838/838 [==============================] - 0s 295us/step - loss: 0.2387 - val_loss: 0.2443\n",
      "Epoch 104/300\n",
      "838/838 [==============================] - 0s 309us/step - loss: 0.2340 - val_loss: 0.2400\n",
      "Epoch 105/300\n",
      "838/838 [==============================] - 0s 314us/step - loss: 0.2325 - val_loss: 0.2457\n",
      "Epoch 106/300\n",
      "838/838 [==============================] - 0s 304us/step - loss: 0.2357 - val_loss: 0.2463\n",
      "Epoch 107/300\n",
      "838/838 [==============================] - 0s 259us/step - loss: 0.2331 - val_loss: 0.2432\n",
      "Epoch 108/300\n",
      "838/838 [==============================] - 0s 259us/step - loss: 0.2337 - val_loss: 0.2457\n",
      "Epoch 109/300\n",
      "838/838 [==============================] - 0s 244us/step - loss: 0.2350 - val_loss: 0.2467\n",
      "Epoch 110/300\n",
      "838/838 [==============================] - 0s 267us/step - loss: 0.2337 - val_loss: 0.2458\n",
      "Epoch 111/300\n",
      "838/838 [==============================] - 0s 284us/step - loss: 0.2326 - val_loss: 0.2440\n",
      "Epoch 112/300\n",
      "838/838 [==============================] - 0s 283us/step - loss: 0.2313 - val_loss: 0.2420\n",
      "Epoch 113/300\n",
      "838/838 [==============================] - 0s 241us/step - loss: 0.2319 - val_loss: 0.2482\n",
      "Epoch 114/300\n",
      "838/838 [==============================] - 0s 352us/step - loss: 0.2317 - val_loss: 0.2463\n",
      "Epoch 115/300\n",
      "838/838 [==============================] - 0s 255us/step - loss: 0.2343 - val_loss: 0.2422\n",
      "Epoch 116/300\n",
      "838/838 [==============================] - 0s 258us/step - loss: 0.2320 - val_loss: 0.2403\n",
      "Epoch 117/300\n",
      "838/838 [==============================] - 0s 241us/step - loss: 0.2333 - val_loss: 0.2456\n",
      "Epoch 118/300\n",
      "838/838 [==============================] - 0s 291us/step - loss: 0.2316 - val_loss: 0.2458\n",
      "Epoch 119/300\n",
      "838/838 [==============================] - 0s 279us/step - loss: 0.2304 - val_loss: 0.2491\n",
      "Epoch 120/300\n",
      "838/838 [==============================] - 0s 267us/step - loss: 0.2295 - val_loss: 0.2462\n",
      "Epoch 121/300\n",
      "838/838 [==============================] - 0s 255us/step - loss: 0.2352 - val_loss: 0.2410\n",
      "Epoch 122/300\n",
      "838/838 [==============================] - 0s 259us/step - loss: 0.2367 - val_loss: 0.2431\n",
      "Epoch 123/300\n",
      "838/838 [==============================] - 0s 252us/step - loss: 0.2285 - val_loss: 0.2448\n",
      "Epoch 124/300\n",
      "838/838 [==============================] - 0s 241us/step - loss: 0.2319 - val_loss: 0.2528\n",
      "Epoch 125/300\n",
      "838/838 [==============================] - 0s 264us/step - loss: 0.2317 - val_loss: 0.2442\n",
      "Epoch 126/300\n",
      "838/838 [==============================] - 0s 258us/step - loss: 0.2297 - val_loss: 0.2496\n",
      "Epoch 127/300\n",
      "838/838 [==============================] - 0s 259us/step - loss: 0.2291 - val_loss: 0.2434\n",
      "Epoch 128/300\n",
      "838/838 [==============================] - 0s 261us/step - loss: 0.2313 - val_loss: 0.2440\n",
      "Epoch 129/300\n",
      "838/838 [==============================] - 0s 254us/step - loss: 0.2297 - val_loss: 0.2449\n",
      "Epoch 130/300\n",
      "838/838 [==============================] - 0s 253us/step - loss: 0.2275 - val_loss: 0.2459\n",
      "Epoch 131/300\n",
      "838/838 [==============================] - 0s 247us/step - loss: 0.2271 - val_loss: 0.2489\n",
      "Epoch 132/300\n",
      "838/838 [==============================] - 0s 255us/step - loss: 0.2290 - val_loss: 0.2534\n",
      "Epoch 133/300\n",
      "838/838 [==============================] - 0s 252us/step - loss: 0.2263 - val_loss: 0.2463\n",
      "Epoch 134/300\n",
      "838/838 [==============================] - 0s 262us/step - loss: 0.2308 - val_loss: 0.2471\n",
      "Epoch 135/300\n",
      "838/838 [==============================] - 0s 255us/step - loss: 0.2276 - val_loss: 0.2469\n",
      "Epoch 136/300\n",
      "838/838 [==============================] - 0s 253us/step - loss: 0.2288 - val_loss: 0.2528\n",
      "Epoch 137/300\n",
      "838/838 [==============================] - 0s 258us/step - loss: 0.2285 - val_loss: 0.2497\n",
      "Epoch 138/300\n",
      "838/838 [==============================] - 0s 297us/step - loss: 0.2288 - val_loss: 0.2470\n",
      "Epoch 139/300\n",
      "838/838 [==============================] - 0s 301us/step - loss: 0.2272 - val_loss: 0.2501\n",
      "Epoch 140/300\n",
      "838/838 [==============================] - 0s 351us/step - loss: 0.2256 - val_loss: 0.2576\n",
      "Epoch 141/300\n",
      "838/838 [==============================] - 0s 301us/step - loss: 0.2293 - val_loss: 0.2474\n",
      "Epoch 142/300\n",
      "838/838 [==============================] - 0s 305us/step - loss: 0.2290 - val_loss: 0.2588\n",
      "Epoch 143/300\n",
      "838/838 [==============================] - 0s 301us/step - loss: 0.2265 - val_loss: 0.2455\n",
      "Epoch 144/300\n",
      "838/838 [==============================] - 0s 278us/step - loss: 0.2246 - val_loss: 0.2523\n",
      "Epoch 145/300\n",
      "838/838 [==============================] - 0s 277us/step - loss: 0.2238 - val_loss: 0.2502\n",
      "Epoch 146/300\n",
      "838/838 [==============================] - 0s 320us/step - loss: 0.2286 - val_loss: 0.2466\n",
      "Epoch 147/300\n",
      "838/838 [==============================] - 0s 298us/step - loss: 0.2276 - val_loss: 0.2491\n",
      "Epoch 148/300\n",
      "838/838 [==============================] - 0s 317us/step - loss: 0.2247 - val_loss: 0.2520\n",
      "Epoch 149/300\n",
      "838/838 [==============================] - 0s 265us/step - loss: 0.2263 - val_loss: 0.2470\n",
      "Epoch 150/300\n",
      "838/838 [==============================] - 0s 264us/step - loss: 0.2222 - val_loss: 0.2470\n",
      "Epoch 151/300\n",
      "838/838 [==============================] - 0s 283us/step - loss: 0.2268 - val_loss: 0.2485\n",
      "Epoch 152/300\n",
      "838/838 [==============================] - 0s 320us/step - loss: 0.2275 - val_loss: 0.2568\n",
      "Epoch 153/300\n",
      "838/838 [==============================] - 0s 256us/step - loss: 0.2270 - val_loss: 0.2493\n",
      "Epoch 154/300\n",
      "838/838 [==============================] - 0s 304us/step - loss: 0.2215 - val_loss: 0.2565\n",
      "Epoch 155/300\n",
      "838/838 [==============================] - 0s 265us/step - loss: 0.2206 - val_loss: 0.2533\n",
      "Epoch 156/300\n",
      "838/838 [==============================] - 0s 296us/step - loss: 0.2196 - val_loss: 0.2474\n",
      "Epoch 157/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "838/838 [==============================] - 0s 260us/step - loss: 0.2221 - val_loss: 0.2466\n",
      "Epoch 158/300\n",
      "838/838 [==============================] - 0s 254us/step - loss: 0.2249 - val_loss: 0.2609\n",
      "Epoch 159/300\n",
      "838/838 [==============================] - 0s 254us/step - loss: 0.2246 - val_loss: 0.2436\n",
      "Epoch 160/300\n",
      "838/838 [==============================] - 0s 267us/step - loss: 0.2198 - val_loss: 0.2532\n",
      "Epoch 161/300\n",
      "838/838 [==============================] - 0s 254us/step - loss: 0.2200 - val_loss: 0.2647\n",
      "Epoch 162/300\n",
      "838/838 [==============================] - 0s 236us/step - loss: 0.2268 - val_loss: 0.2679\n",
      "Epoch 163/300\n",
      "838/838 [==============================] - 0s 264us/step - loss: 0.2231 - val_loss: 0.2560\n",
      "Epoch 164/300\n",
      "838/838 [==============================] - 0s 254us/step - loss: 0.2189 - val_loss: 0.2462\n",
      "Epoch 165/300\n",
      "838/838 [==============================] - 0s 235us/step - loss: 0.2159 - val_loss: 0.2690\n",
      "Epoch 166/300\n",
      "838/838 [==============================] - 0s 258us/step - loss: 0.2176 - val_loss: 0.2519\n",
      "Epoch 167/300\n",
      "838/838 [==============================] - 0s 253us/step - loss: 0.2182 - val_loss: 0.2615\n",
      "Epoch 168/300\n",
      "838/838 [==============================] - 0s 241us/step - loss: 0.2185 - val_loss: 0.2534\n",
      "Epoch 169/300\n",
      "838/838 [==============================] - 0s 236us/step - loss: 0.2188 - val_loss: 0.2503\n",
      "Epoch 170/300\n",
      "838/838 [==============================] - 0s 228us/step - loss: 0.2226 - val_loss: 0.2633\n",
      "Epoch 171/300\n",
      "838/838 [==============================] - 0s 241us/step - loss: 0.2211 - val_loss: 0.2424\n",
      "Epoch 172/300\n",
      "838/838 [==============================] - 0s 239us/step - loss: 0.2142 - val_loss: 0.2573\n",
      "Epoch 173/300\n",
      "838/838 [==============================] - 0s 249us/step - loss: 0.2194 - val_loss: 0.2671\n",
      "Epoch 174/300\n",
      "838/838 [==============================] - 0s 240us/step - loss: 0.2245 - val_loss: 0.2438\n",
      "Epoch 175/300\n",
      "838/838 [==============================] - 0s 246us/step - loss: 0.2155 - val_loss: 0.2557\n",
      "Epoch 176/300\n",
      "838/838 [==============================] - 0s 243us/step - loss: 0.2140 - val_loss: 0.2528\n",
      "Epoch 177/300\n",
      "838/838 [==============================] - 0s 248us/step - loss: 0.2153 - val_loss: 0.2528\n",
      "Epoch 178/300\n",
      "838/838 [==============================] - 0s 248us/step - loss: 0.2164 - val_loss: 0.2733\n",
      "Epoch 179/300\n",
      "838/838 [==============================] - 0s 240us/step - loss: 0.2355 - val_loss: 0.2359\n",
      "Epoch 180/300\n",
      "838/838 [==============================] - 0s 240us/step - loss: 0.2194 - val_loss: 0.2407\n",
      "Epoch 181/300\n",
      "838/838 [==============================] - 0s 241us/step - loss: 0.2178 - val_loss: 0.2516\n",
      "Epoch 182/300\n",
      "838/838 [==============================] - 0s 250us/step - loss: 0.2161 - val_loss: 0.2509\n",
      "Epoch 183/300\n",
      "838/838 [==============================] - 0s 249us/step - loss: 0.2090 - val_loss: 0.2540\n",
      "Epoch 184/300\n",
      "838/838 [==============================] - 0s 241us/step - loss: 0.2135 - val_loss: 0.2673\n",
      "Epoch 185/300\n",
      "838/838 [==============================] - 0s 249us/step - loss: 0.2234 - val_loss: 0.2475\n",
      "Epoch 186/300\n",
      "838/838 [==============================] - 0s 241us/step - loss: 0.2089 - val_loss: 0.2482\n",
      "Epoch 187/300\n",
      "838/838 [==============================] - 0s 241us/step - loss: 0.2087 - val_loss: 0.2597\n",
      "Epoch 188/300\n",
      "838/838 [==============================] - 0s 249us/step - loss: 0.2136 - val_loss: 0.2457\n",
      "Epoch 189/300\n",
      "838/838 [==============================] - 0s 252us/step - loss: 0.2079 - val_loss: 0.2435\n",
      "Epoch 190/300\n",
      "838/838 [==============================] - 0s 247us/step - loss: 0.2171 - val_loss: 0.2583\n",
      "Epoch 191/300\n",
      "838/838 [==============================] - 0s 242us/step - loss: 0.2151 - val_loss: 0.2505\n",
      "Epoch 192/300\n",
      "838/838 [==============================] - 0s 248us/step - loss: 0.2135 - val_loss: 0.2613\n",
      "Epoch 193/300\n",
      "838/838 [==============================] - 0s 230us/step - loss: 0.2145 - val_loss: 0.2861\n",
      "Epoch 194/300\n",
      "838/838 [==============================] - 0s 242us/step - loss: 0.2240 - val_loss: 0.2585\n",
      "Epoch 195/300\n",
      "838/838 [==============================] - 0s 240us/step - loss: 0.2112 - val_loss: 0.2538\n",
      "Epoch 196/300\n",
      "838/838 [==============================] - 0s 234us/step - loss: 0.2075 - val_loss: 0.2396\n",
      "Epoch 197/300\n",
      "838/838 [==============================] - 0s 253us/step - loss: 0.2106 - val_loss: 0.2550\n",
      "Epoch 198/300\n",
      "838/838 [==============================] - 0s 241us/step - loss: 0.2187 - val_loss: 0.2550\n",
      "Epoch 199/300\n",
      "838/838 [==============================] - 0s 240us/step - loss: 0.2059 - val_loss: 0.2529\n",
      "Epoch 200/300\n",
      "838/838 [==============================] - 0s 235us/step - loss: 0.2060 - val_loss: 0.2647\n",
      "Epoch 201/300\n",
      "838/838 [==============================] - 0s 242us/step - loss: 0.2101 - val_loss: 0.2735\n",
      "Epoch 202/300\n",
      "838/838 [==============================] - 0s 250us/step - loss: 0.2059 - val_loss: 0.2564\n",
      "Epoch 203/300\n",
      "838/838 [==============================] - 0s 256us/step - loss: 0.2107 - val_loss: 0.2530\n",
      "Epoch 204/300\n",
      "838/838 [==============================] - 0s 243us/step - loss: 0.2057 - val_loss: 0.2781\n",
      "Epoch 205/300\n",
      "838/838 [==============================] - 0s 247us/step - loss: 0.2174 - val_loss: 0.2592\n",
      "Epoch 206/300\n",
      "838/838 [==============================] - 0s 246us/step - loss: 0.2061 - val_loss: 0.2600\n",
      "Epoch 207/300\n",
      "838/838 [==============================] - 0s 261us/step - loss: 0.2063 - val_loss: 0.2623\n",
      "Epoch 208/300\n",
      "838/838 [==============================] - 0s 244us/step - loss: 0.2076 - val_loss: 0.2570\n",
      "Epoch 209/300\n",
      "838/838 [==============================] - 0s 243us/step - loss: 0.2047 - val_loss: 0.2553\n",
      "Epoch 210/300\n",
      "838/838 [==============================] - 0s 243us/step - loss: 0.2178 - val_loss: 0.2564\n",
      "Epoch 211/300\n",
      "838/838 [==============================] - 0s 249us/step - loss: 0.2225 - val_loss: 0.2511\n",
      "Epoch 212/300\n",
      "838/838 [==============================] - 0s 259us/step - loss: 0.2057 - val_loss: 0.2628\n",
      "Epoch 213/300\n",
      "838/838 [==============================] - 0s 256us/step - loss: 0.2018 - val_loss: 0.2499\n",
      "Epoch 214/300\n",
      "838/838 [==============================] - 0s 248us/step - loss: 0.2041 - val_loss: 0.2414\n",
      "Epoch 215/300\n",
      "838/838 [==============================] - 0s 244us/step - loss: 0.2102 - val_loss: 0.2450\n",
      "Epoch 216/300\n",
      "838/838 [==============================] - 0s 259us/step - loss: 0.2075 - val_loss: 0.2708\n",
      "Epoch 217/300\n",
      "838/838 [==============================] - 0s 258us/step - loss: 0.2054 - val_loss: 0.2710\n",
      "Epoch 218/300\n",
      "838/838 [==============================] - 0s 270us/step - loss: 0.2030 - val_loss: 0.2561\n",
      "Epoch 219/300\n",
      "838/838 [==============================] - 0s 242us/step - loss: 0.1991 - val_loss: 0.2590\n",
      "Epoch 220/300\n",
      "838/838 [==============================] - 0s 250us/step - loss: 0.2106 - val_loss: 0.2726\n",
      "Epoch 221/300\n",
      "838/838 [==============================] - 0s 250us/step - loss: 0.2041 - val_loss: 0.2497\n",
      "Epoch 222/300\n",
      "838/838 [==============================] - 0s 246us/step - loss: 0.2046 - val_loss: 0.2702\n",
      "Epoch 223/300\n",
      "838/838 [==============================] - 0s 252us/step - loss: 0.2121 - val_loss: 0.2744\n",
      "Epoch 224/300\n",
      "838/838 [==============================] - 0s 244us/step - loss: 0.2033 - val_loss: 0.2632\n",
      "Epoch 225/300\n",
      "838/838 [==============================] - 0s 279us/step - loss: 0.2005 - val_loss: 0.2464\n",
      "Epoch 226/300\n",
      "838/838 [==============================] - 0s 254us/step - loss: 0.2101 - val_loss: 0.2549\n",
      "Epoch 227/300\n",
      "838/838 [==============================] - 0s 252us/step - loss: 0.1997 - val_loss: 0.2616\n",
      "Epoch 228/300\n",
      "838/838 [==============================] - 0s 261us/step - loss: 0.2036 - val_loss: 0.2715\n",
      "Epoch 229/300\n",
      "838/838 [==============================] - 0s 255us/step - loss: 0.2057 - val_loss: 0.2717\n",
      "Epoch 230/300\n",
      "838/838 [==============================] - 0s 258us/step - loss: 0.2087 - val_loss: 0.2565\n",
      "Epoch 231/300\n",
      "838/838 [==============================] - 0s 246us/step - loss: 0.1971 - val_loss: 0.2537\n",
      "Epoch 232/300\n",
      "838/838 [==============================] - 0s 249us/step - loss: 0.1990 - val_loss: 0.2592\n",
      "Epoch 233/300\n",
      "838/838 [==============================] - 0s 247us/step - loss: 0.2108 - val_loss: 0.2666\n",
      "Epoch 234/300\n",
      "838/838 [==============================] - 0s 246us/step - loss: 0.2028 - val_loss: 0.2734\n",
      "Epoch 235/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "838/838 [==============================] - 0s 243us/step - loss: 0.2035 - val_loss: 0.2748\n",
      "Epoch 236/300\n",
      "838/838 [==============================] - 0s 237us/step - loss: 0.1946 - val_loss: 0.2657\n",
      "Epoch 237/300\n",
      "838/838 [==============================] - 0s 240us/step - loss: 0.1939 - val_loss: 0.2795\n",
      "Epoch 238/300\n",
      "838/838 [==============================] - 0s 246us/step - loss: 0.1972 - val_loss: 0.2803\n",
      "Epoch 239/300\n",
      "838/838 [==============================] - 0s 243us/step - loss: 0.2001 - val_loss: 0.2554\n",
      "Epoch 240/300\n",
      "838/838 [==============================] - 0s 233us/step - loss: 0.2072 - val_loss: 0.2816\n",
      "Epoch 241/300\n",
      "838/838 [==============================] - 0s 235us/step - loss: 0.2024 - val_loss: 0.2577\n",
      "Epoch 242/300\n",
      "838/838 [==============================] - 0s 229us/step - loss: 0.1932 - val_loss: 0.2492\n",
      "Epoch 243/300\n",
      "838/838 [==============================] - 0s 230us/step - loss: 0.1946 - val_loss: 0.2769\n",
      "Epoch 244/300\n",
      "838/838 [==============================] - 0s 240us/step - loss: 0.1899 - val_loss: 0.2837\n",
      "Epoch 245/300\n",
      "838/838 [==============================] - 0s 233us/step - loss: 0.2037 - val_loss: 0.2712\n",
      "Epoch 246/300\n",
      "838/838 [==============================] - 0s 236us/step - loss: 0.1972 - val_loss: 0.2804\n",
      "Epoch 247/300\n",
      "838/838 [==============================] - 0s 241us/step - loss: 0.2018 - val_loss: 0.2403\n",
      "Epoch 248/300\n",
      "838/838 [==============================] - 0s 246us/step - loss: 0.1966 - val_loss: 0.2786\n",
      "Epoch 249/300\n",
      "838/838 [==============================] - 0s 248us/step - loss: 0.2045 - val_loss: 0.2559\n",
      "Epoch 250/300\n",
      "838/838 [==============================] - 0s 244us/step - loss: 0.1932 - val_loss: 0.2839\n",
      "Epoch 251/300\n",
      "838/838 [==============================] - 0s 231us/step - loss: 0.2012 - val_loss: 0.2730\n",
      "Epoch 252/300\n",
      "838/838 [==============================] - 0s 212us/step - loss: 0.1927 - val_loss: 0.2721\n",
      "Epoch 253/300\n",
      "838/838 [==============================] - 0s 228us/step - loss: 0.1915 - val_loss: 0.2758\n",
      "Epoch 254/300\n",
      "838/838 [==============================] - 0s 215us/step - loss: 0.1864 - val_loss: 0.2894\n",
      "Epoch 255/300\n",
      "838/838 [==============================] - 0s 275us/step - loss: 0.1977 - val_loss: 0.2886\n",
      "Epoch 256/300\n",
      "838/838 [==============================] - 0s 237us/step - loss: 0.1881 - val_loss: 0.2691\n",
      "Epoch 257/300\n",
      "838/838 [==============================] - 0s 248us/step - loss: 0.1933 - val_loss: 0.2778\n",
      "Epoch 258/300\n",
      "838/838 [==============================] - 0s 242us/step - loss: 0.1941 - val_loss: 0.2689\n",
      "Epoch 259/300\n",
      "838/838 [==============================] - 0s 250us/step - loss: 0.1921 - val_loss: 0.2735\n",
      "Epoch 260/300\n",
      "838/838 [==============================] - 0s 249us/step - loss: 0.1866 - val_loss: 0.2718\n",
      "Epoch 261/300\n",
      "838/838 [==============================] - 0s 250us/step - loss: 0.1852 - val_loss: 0.2785\n",
      "Epoch 262/300\n",
      "838/838 [==============================] - 0s 252us/step - loss: 0.1948 - val_loss: 0.2463\n",
      "Epoch 263/300\n",
      "838/838 [==============================] - 0s 222us/step - loss: 0.2001 - val_loss: 0.2739\n",
      "Epoch 264/300\n",
      "838/838 [==============================] - 0s 221us/step - loss: 0.1986 - val_loss: 0.2943\n",
      "Epoch 265/300\n",
      "838/838 [==============================] - 0s 271us/step - loss: 0.1977 - val_loss: 0.2814\n",
      "Epoch 266/300\n",
      "838/838 [==============================] - 0s 243us/step - loss: 0.1831 - val_loss: 0.2769\n",
      "Epoch 267/300\n",
      "838/838 [==============================] - 0s 239us/step - loss: 0.1792 - val_loss: 0.2990\n",
      "Epoch 268/300\n",
      "838/838 [==============================] - 0s 259us/step - loss: 0.1952 - val_loss: 0.2805\n",
      "Epoch 269/300\n",
      "838/838 [==============================] - 0s 265us/step - loss: 0.1767 - val_loss: 0.2632\n",
      "Epoch 270/300\n",
      "838/838 [==============================] - 0s 249us/step - loss: 0.1890 - val_loss: 0.2562\n",
      "Epoch 271/300\n",
      "838/838 [==============================] - 0s 241us/step - loss: 0.1923 - val_loss: 0.2746\n",
      "Epoch 272/300\n",
      "838/838 [==============================] - 0s 252us/step - loss: 0.1760 - val_loss: 0.2591\n",
      "Epoch 273/300\n",
      "838/838 [==============================] - 0s 246us/step - loss: 0.1893 - val_loss: 0.2867\n",
      "Epoch 274/300\n",
      "838/838 [==============================] - 0s 248us/step - loss: 0.1815 - val_loss: 0.2830\n",
      "Epoch 275/300\n",
      "838/838 [==============================] - 0s 231us/step - loss: 0.1890 - val_loss: 0.2702\n",
      "Epoch 276/300\n",
      "838/838 [==============================] - 0s 223us/step - loss: 0.1808 - val_loss: 0.2667\n",
      "Epoch 277/300\n",
      "838/838 [==============================] - 0s 225us/step - loss: 0.1854 - val_loss: 0.2509\n",
      "Epoch 278/300\n",
      "838/838 [==============================] - 0s 229us/step - loss: 0.1850 - val_loss: 0.2610\n",
      "Epoch 279/300\n",
      "838/838 [==============================] - 0s 271us/step - loss: 0.1893 - val_loss: 0.2479\n",
      "Epoch 280/300\n",
      "838/838 [==============================] - 0s 235us/step - loss: 0.1970 - val_loss: 0.2862\n",
      "Epoch 281/300\n",
      "838/838 [==============================] - 0s 249us/step - loss: 0.1849 - val_loss: 0.2860\n",
      "Epoch 282/300\n",
      "838/838 [==============================] - 0s 260us/step - loss: 0.1780 - val_loss: 0.2883\n",
      "Epoch 283/300\n",
      "838/838 [==============================] - 0s 243us/step - loss: 0.1890 - val_loss: 0.3008\n",
      "Epoch 284/300\n",
      "838/838 [==============================] - 0s 253us/step - loss: 0.1888 - val_loss: 0.2913\n",
      "Epoch 285/300\n",
      "838/838 [==============================] - 0s 248us/step - loss: 0.1753 - val_loss: 0.2741\n",
      "Epoch 286/300\n",
      "838/838 [==============================] - 0s 237us/step - loss: 0.1701 - val_loss: 0.2950\n",
      "Epoch 287/300\n",
      "838/838 [==============================] - 0s 234us/step - loss: 0.1801 - val_loss: 0.2700\n",
      "Epoch 288/300\n",
      "838/838 [==============================] - 0s 278us/step - loss: 0.1813 - val_loss: 0.2843\n",
      "Epoch 289/300\n",
      "838/838 [==============================] - 0s 280us/step - loss: 0.1733 - val_loss: 0.2902\n",
      "Epoch 290/300\n",
      "838/838 [==============================] - 0s 256us/step - loss: 0.1762 - val_loss: 0.2955\n",
      "Epoch 291/300\n",
      "838/838 [==============================] - 0s 255us/step - loss: 0.1717 - val_loss: 0.2786\n",
      "Epoch 292/300\n",
      "838/838 [==============================] - 0s 240us/step - loss: 0.1712 - val_loss: 0.2826\n",
      "Epoch 293/300\n",
      "838/838 [==============================] - 0s 262us/step - loss: 0.1844 - val_loss: 0.3011\n",
      "Epoch 294/300\n",
      "838/838 [==============================] - 0s 254us/step - loss: 0.1812 - val_loss: 0.2850\n",
      "Epoch 295/300\n",
      "838/838 [==============================] - 0s 250us/step - loss: 0.1685 - val_loss: 0.2690\n",
      "Epoch 296/300\n",
      "838/838 [==============================] - 0s 241us/step - loss: 0.1774 - val_loss: 0.3013\n",
      "Epoch 297/300\n",
      "838/838 [==============================] - 0s 248us/step - loss: 0.1883 - val_loss: 0.2697\n",
      "Epoch 298/300\n",
      "838/838 [==============================] - 0s 249us/step - loss: 0.1787 - val_loss: 0.2661\n",
      "Epoch 299/300\n",
      "838/838 [==============================] - 0s 250us/step - loss: 0.1864 - val_loss: 0.2808\n",
      "Epoch 300/300\n",
      "838/838 [==============================] - 0s 256us/step - loss: 0.1965 - val_loss: 0.2893\n",
      "[0.37997434 0.62002563 0.13078116 0.8692189  0.36809656 0.63190347\n",
      " 0.89753586 0.10246418 0.6670107  0.33298925 0.5229421  0.47705787\n",
      " 0.71713835 0.28286168 0.3984501  0.6015499  0.6105404  0.3894596\n",
      " 0.32184884 0.6781512  0.764844   0.23515598 0.01690131 0.9830987\n",
      " 0.42717534 0.57282466 0.5435005  0.45649952 0.5147417  0.4852583\n",
      " 0.6562713  0.34372875 0.98925716 0.01074281 0.7604739  0.23952612\n",
      " 0.4249609  0.57503915 0.6213533  0.37864667 0.17873073 0.8212693\n",
      " 0.7497856  0.2502144  0.23069197 0.76930803 0.58843684 0.41156316\n",
      " 0.91049767 0.08950239 0.39588875 0.60411125 0.6512607  0.34873933\n",
      " 0.3700814  0.62991863 0.52884555 0.4711545  0.7385827  0.26141727\n",
      " 0.7743604  0.22563957 0.12393928 0.8760607  0.5508018  0.4491982\n",
      " 0.5426596  0.45734036 0.22501107 0.77498895 0.08087336 0.9191267\n",
      " 0.3317884  0.66821164 0.29861778 0.70138216 0.08163889 0.9183611\n",
      " 0.5151787  0.48482135 0.6256724  0.37432754 0.11045218 0.8895479\n",
      " 0.5440405  0.4559596  0.1219055  0.8780945  0.6081719  0.39182812\n",
      " 0.3735358  0.6264641  0.40315068 0.5968493  0.48644513 0.5135549\n",
      " 0.56892645 0.43107352 0.52758074 0.47241917 0.6422081  0.35779193\n",
      " 0.6419242  0.35807583 0.7053111  0.29468885 0.86515164 0.13484839\n",
      " 0.40172338 0.59827656 0.30353695 0.69646305 0.4552251  0.5447749\n",
      " 0.6693775  0.3306225  0.5459914  0.45400858 0.54970586 0.45029417\n",
      " 0.42793187 0.57206815 0.20613925 0.79386073 0.14217567 0.8578244\n",
      " 0.01349024 0.98650974 0.68901235 0.31098762 0.57504404 0.42495602\n",
      " 0.37686178 0.6231382  0.2354363  0.76456374 0.57315606 0.42684388\n",
      " 0.70595074 0.29404926 0.27421552 0.72578454 0.95415944 0.04584062\n",
      " 0.37909052 0.62090945 0.8252874  0.17471264 0.5900345  0.40996554\n",
      " 0.72848123 0.27151874 0.15914819 0.84085184 0.23521763 0.76478237\n",
      " 0.6082073  0.39179268 0.41811812 0.5818819  0.44998112 0.5500189\n",
      " 0.68871963 0.31128034 0.54935753 0.4506425  0.11295174 0.8870483\n",
      " 0.6247801  0.37521988 0.53267723 0.46732277 0.5572359  0.4427641\n",
      " 0.2639613  0.7360387  0.86929715 0.1307029  0.6859469  0.31405315\n",
      " 0.7320011  0.2679988  0.1349908  0.8650092  0.14235312 0.8576469\n",
      " 0.56895477 0.43104517 0.6718805  0.32811952 0.26789176 0.73210824\n",
      " 0.4098655  0.5901345  0.22476372 0.7752363  0.7241766  0.27582347\n",
      " 0.45573428 0.54426575 0.14167508 0.85832494 0.4475936  0.55240643\n",
      " 0.4381724  0.56182754 0.3365133  0.66348666 0.5037147  0.49628535\n",
      " 0.9651473  0.03485261 0.38199133 0.61800873 0.03171599 0.9682841\n",
      " 0.14799878 0.8520012  0.2752609  0.724739   0.15507448 0.8449255\n",
      " 0.18340383 0.81659615 0.6295112  0.3704888  0.41884106 0.5811589\n",
      " 0.05440653 0.9455934  0.5350873  0.46491268 0.3401084  0.6598916\n",
      " 0.58350223 0.41649783 0.23966002 0.76034    0.52489525 0.4751048\n",
      " 0.00392285 0.9960771  0.14663827 0.8533617  0.4102484  0.5897516\n",
      " 0.73874223 0.26125777 0.11461622 0.88538384 0.34140956 0.6585904\n",
      " 0.46536058 0.5346394  0.69929487 0.30070516 0.14939873 0.8506012\n",
      " 0.18954745 0.8104526  0.16505142 0.83494854 0.6253501  0.37464985\n",
      " 0.8802582  0.11974176 0.35833967 0.64166033 0.6807259  0.31927416\n",
      " 0.6746275  0.3253725  0.67911214 0.32088786 0.49612862 0.5038714\n",
      " 0.12298641 0.8770136  0.43196946 0.56803054 0.5067415  0.4932585\n",
      " 0.8400929  0.15990716 0.17447811 0.8255218  0.62877566 0.37122434\n",
      " 0.4051431  0.5948569  0.26477766 0.7352224  0.06827383 0.93172616\n",
      " 0.98972565 0.01027432 0.64445984 0.35554013 0.11777905 0.8822209\n",
      " 0.0310186  0.96898144 0.8368929  0.16310711 0.3969266  0.6030734\n",
      " 0.0559442  0.9440558  0.3697639  0.6302361  0.56484723 0.4351528\n",
      " 0.53498757 0.46501246 0.67046374 0.3295363  0.5537217  0.44627824\n",
      " 0.69045365 0.30954632 0.4176498  0.5823502  0.19964987 0.8003501\n",
      " 0.07703785 0.9229621  0.1647204  0.83527964 0.9178349  0.08216512\n",
      " 0.34392232 0.6560777  0.30516174 0.6948383  0.48189887 0.5181011\n",
      " 0.589894   0.410106   0.41563785 0.58436215 0.4735977  0.5264023\n",
      " 0.4846664  0.5153336  0.31391633 0.6860837  0.70448536 0.29551464\n",
      " 0.51783633 0.48216364 0.63034666 0.36965337 0.3924292  0.60757077\n",
      " 0.5445047  0.45549533 0.15801345 0.8419865  0.64649177 0.35350826\n",
      " 0.79138035 0.2086197  0.80543107 0.19456898 0.47287273 0.52712727\n",
      " 0.5458286  0.45417145 0.65586174 0.34413826 0.90415967 0.09584036\n",
      " 0.21775793 0.7822421  0.70949095 0.29050905 0.13791043 0.8620895\n",
      " 0.7256433  0.27435672 0.6534193  0.3465806  0.3543867  0.64561325\n",
      " 0.39271492 0.6072851  0.1206722  0.87932783 0.395934   0.604066\n",
      " 0.5911147  0.4088853  0.93449587 0.06550414 0.44196805 0.558032\n",
      " 0.50533783 0.49466214 0.6053651  0.3946349  0.58697706 0.4130229\n",
      " 0.14257243 0.85742754 0.28788564 0.7121144  0.13924184 0.8607581\n",
      " 0.07528535 0.9247146  0.5547388  0.44526118 0.6124733  0.38752666\n",
      " 0.83276564 0.16723438 0.38051653 0.6194834  0.5948792  0.4051208\n",
      " 0.57482606 0.42517394 0.5689212  0.43107885 0.7368457  0.26315436\n",
      " 0.5590556  0.4409444  0.3693529  0.6306471  0.4511964  0.5488035\n",
      " 0.36134976 0.63865024 0.7589264  0.24107352 0.44622892 0.55377114\n",
      " 0.39287394 0.607126   0.06895639 0.9310436  0.5529888  0.44701123\n",
      " 0.09356475 0.90643525 0.9041466  0.0958534  0.21684667 0.7831533\n",
      " 0.25122803 0.748772   0.49947432 0.5005257  0.49530107 0.50469893\n",
      " 0.7106982  0.28930184 0.59495467 0.40504533 0.06067968 0.9393204\n",
      " 0.47208485 0.5279151  0.20786315 0.79213685 0.45850676 0.54149324]\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 0. 0. 1. 0. 1. 1. 1. 1. 0. 1. 0. 1. 0.\n",
      " 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1.\n",
      " 1. 0. 1. 1. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 1. 0. 1. 0. 0. 0.\n",
      " 0. 1. 1. 1. 1. 0. 1. 0. 0. 0. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0.\n",
      " 1. 1. 0. 1. 1. 1. 1. 0. 0. 1. 0. 1. 1. 0. 1. 0. 1. 0. 1. 1. 0. 1. 0. 0.\n",
      " 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 0. 1. 1. 0. 1. 0. 1. 0. 0.\n",
      " 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0.\n",
      " 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 0. 1. 0. 1. 0.\n",
      " 0. 1. 1. 0. 0. 1. 1. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_x, train_y, test_x, test_y, scaler = load_data(test)\n",
    "train_x = np.reshape(train_x, (train_x.shape[0], train_x.shape[1], dimensions))\n",
    "test_x = np.reshape(test_x, (test_x.shape[0], test_x.shape[1], dimensions))\n",
    "predict_y, test_y = train_model(train_x, train_y, test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "indicator = list()\n",
    "count = 0\n",
    "for i in range(1, len(predict_y), 2):\n",
    "    count += 1\n",
    "    indicator.append(1 if predict_y[i] >= predict_y[i - 1] else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 122\n",
      "% Right: 0.5213675213675214\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "plotter = list()\n",
    "for i in range(len(indicator)):\n",
    "    if indicator[i] == test_y[i]:\n",
    "        correct += 1\n",
    "        plotter.append(1)\n",
    "    else:\n",
    "        plotter.append(0)\n",
    "\n",
    "print(\"Correct: \" + str(correct))\n",
    "print(\"% Right: \" + str(correct/len(indicator)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
